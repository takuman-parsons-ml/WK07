{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 07\n",
        "\n",
        "Supervised Learning: Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from data_utils import StandardScaler\n",
        "from data_utils import LinearRegression\n",
        "from data_utils import SGDClassifier, RandomForestClassifier, SVC, LogisticRegression\n",
        "from data_utils import display_confusion_matrix, regression_error\n",
        "from data_utils import object_from_json_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKGt8bzScNA"
      },
      "source": [
        "## Regression\n",
        "\n",
        "Regression, or Regression Analysis, is a set of statistical processes for estimating the relationship between a dependent variable (sometimes called the 'outcome', 'response' or 'label') and one or more independent variables (called 'features', 'dimensions' or 'columns').\n",
        "\n",
        "For example, let's say we have the following data about people's wages and years of experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp.png\" width=\"620px\"/>\n",
        "\n",
        "We could use regression to calculate how the values for wages are affected by years of experience in our dataset, and then create a function to more generally estimate the relation between wages and experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "We could now estimate wages for values of years of experience that we didn't have measurements for.\n",
        "\n",
        "This is an estimate, but the more points we use and the more features we have in our dataset the better the regression results will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Regression!\n",
        "\n",
        "Let's do one more regression exercise using a different dataset.\n",
        "\n",
        "This one is for wine quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "# Look at features: values, types, names, etc\n",
        "wines_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze the data\n",
        "\n",
        "Specifically, steps $3$: normalize and look at the covariance matrix using `quality` as the independent variable of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: normalize and look at covariance matrix\n",
        "\n",
        "## 3. Normalize\n",
        "scaler = StandardScaler()\n",
        "\n",
        "wines_scaled = scaler.fit_transform(wines_df)\n",
        "\n",
        "# Since this is a new dataset, let's peek at its covariance matrix\n",
        "\n",
        "wines_scaled.cov()[\"quality\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot\n",
        "\n",
        "Looks like `alcohol`, `density`, `chlorides` and `volatility` are the $4$ features that mostly contribute to the quality.\n",
        "\n",
        "Let's look at graphs of `quality` as a function of these $4$ features.\n",
        "\n",
        "This could be two $3D$ graphs of pairs of variables, but four $2D$ graphs is probably easier to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot output feature as a function of input features\n",
        "def plot_feats(data, in_feats, out_feat):\n",
        "  for feat in in_feats:\n",
        "    x = data[feat]\n",
        "    y = data[out_feat]\n",
        "    plt.plot(x, y, marker='o', linestyle='', alpha=0.3)\n",
        "    plt.xlabel(feat)\n",
        "    plt.ylabel(out_feat)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_list = [\"alcohol\", \"density\", \"chlorides\", \"volatility\"]\n",
        "plot_feats(wines_scaled, feat_list, \"quality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression\n",
        "\n",
        "Use the method we used last week in the diamond dataset to create a model that predicts wine `quality` as a function of **ALL** of its other features.\n",
        "\n",
        "Use all of our features to run regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a linear model and run regression\n",
        "\n",
        "## 4. Separate the outcome variable and the independent variables\n",
        "feats = wines_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "model_lin = LinearRegression()\n",
        "\n",
        "# Create a model that relates quality of wines to all other features\n",
        "model_lin.fit(feats, wines_scaled[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "pred_train_scaled = model_lin.predict(feats)\n",
        "\n",
        "# Un-normalize the data\n",
        "pred_train = scaler.inverse_transform(pred_train_scaled).apply(round)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_df[\"quality\"], pred_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "On average our predictions are within $0.75$ points of the real quality values.\n",
        "\n",
        "Save the original `quality` values in a variable called `quality_original` and the `predicted quality` in another variable, called`quality_predicted`, and run the cell below to look at some plots of our predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_preds(data, prediction, in_feats, out_feat):\n",
        "  for feat in in_feats:\n",
        "    x = data[feat]\n",
        "    y = data[out_feat]\n",
        "    y_p = prediction[out_feat]\n",
        "\n",
        "    # Plot the original quality\n",
        "    plt.plot(x, y, marker='o', linestyle='', alpha=0.3)\n",
        "    plt.plot(x, y_p, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "    plt.xlabel(feat)\n",
        "    plt.ylabel(out_feat)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_list = [\"alcohol\", \"density\", \"chlorides\", \"volatility\"]\n",
        "plot_preds(wines_df, pred_train, feat_list, \"quality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "# 🤔\n",
        "\n",
        "Hmm.... these could be better.\n",
        "\n",
        "Our model wasn't able to capture the fact that the calculated `quality` value should be a discrete value and not a number with decimals.\n",
        "\n",
        "This is because our `quality` category is not continuous, and instead can only have particular discrete values.\n",
        "\n",
        "Instead of trying to calculate continuous values for `quality`, our model should really be trying to put the wines in the right `quality` category.\n",
        "\n",
        "Let's use a different type of model for this task.\n",
        "\n",
        "Instead of learning how to predict a continuous value from the independent variables, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "Our model should learn how to place data points into discrete groups, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-classes.png\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification\n",
        "\n",
        "This is what's called a *classification* problem, or task (sometimes also called _Logistic Regression_).\n",
        "\n",
        "Instead of trying to model the behavior of a continuous value, like `price` or `temperature`, a classification model tries to predict the correct *label* for given input data.\n",
        "\n",
        "The steps for training a classification model are the same:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Separate the outcome variable and the feature variables\n",
        "5. Create a model\n",
        "6. Run model on input data and test data, and \n",
        "7. Measure error\n",
        "\n",
        "Even though we are trying to predict labels for our data, we still have to encode all label/categorical features into numbers, whether they are input or output variables. This is because our models are always using some kind of math to compare values and make predictions, and they wouldn't really know how to do that with text or other non-numeric data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random ? Forest ?\n",
        "\n",
        "The particular classification model we will use is called a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "The model gets its name from another type of model called a [Decision Tree](https://scikit-learn.org/stable/modules/tree.html). During training, Decision Trees learn to model our data using simple decision rules, in a process that is conceptually similar to the game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_questions).\n",
        "\n",
        "It learns to separate our data into categories by *asking* a series of yes/no, if/else, questions using our features:\n",
        "\n",
        "<img src=\"./imgs/decision-trees.jpg\" width=\"720px\"/>\n",
        "\n",
        "A Random Forest Classifier is a model that combines a bunch of Tree models that were trained with randomly selected subsets of our features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's Model !\n",
        "\n",
        "Steps are the same as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "\n",
        "# We're still using scaled feature variables\n",
        "\n",
        "# But, now our quality variable will be unscaled,\n",
        "# so it keeps its original values as labels 0 - 9\n",
        "\n",
        "## 5. Create a RandomForestClassifier object\n",
        "model_rf = RandomForestClassifier(random_state=1010)\n",
        "\n",
        "# Create a model that classifies quality of wines based on many features\n",
        "model_rf.fit(feats, wines_df[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_train = model_rf.predict(feats)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_df[\"quality\"], predicted_train[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "Ohh, that's better. On average, the model misses the quality value by about $0.07$ points.\n",
        "\n",
        "Let's take a look at some plots to confirm that the model captured the discrete-ness of our label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_preds(wines_df, predicted_train, feat_list, \"quality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🍷🎉\n",
        "\n",
        "Much better !\n",
        "\n",
        "### Test Dataset\n",
        "\n",
        "One thing that we haven't been doing, but is an important step when training any kind of ML model, is to actually test our model on data that wasn't used in the training.\n",
        "\n",
        "When we calculate the regression/prediction error on the same dataset that we use to create the model, it tells us how well the model learned from the data. When we calculate this error on a test dataset, one that wasn't used for fitting the model, it tells us how well the model can generalize its predictions to data it hasn't seen.\n",
        "\n",
        "The regression/prediction error is an indication of how good our model is.\n",
        "\n",
        "Calculating it using a separate test dataset will tell us whether our model has actually learned patterns from the data or just memorized the training data.\n",
        "\n",
        "The steps for preparing the data and running it through the model are the same, except we don't have to `fit()` a scaler, nor a model. We'll just use the scaler and model that we trained with the `train` set to `transform()`/`predict()` the `test` data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_TEST_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/wines-test.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_test_data = object_from_json_url(WINE_TEST_FILE)\n",
        "wines_test_df = pd.DataFrame.from_records(wines_test_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wines_test_scaled = scaler.transform(wines_test_df)\n",
        "\n",
        "## 4. Separate the independent variables\n",
        "features_test = wines_test_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "predicted_test = model_rf.predict(features_test)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_test_df[\"quality\"], predicted_test[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "This is not great. On average our model is within $0.7$ quality points of the real values.\n",
        "\n",
        "Let's take a look at some plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_preds(wines_test_df, predicted_test, feat_list, \"quality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Better Interpretation\n",
        "\n",
        "Using the `regression_error()` function like this to measure the error of our classifier model gives us some sense of what's happening in the model, but we have better ways of analyzing the results of a classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "The **_Confusion Matrix_** graph is an example of an evaluation metric that is specifically used for measuring how well a classification model performs.\n",
        "\n",
        "It's a very elegant way of displaying which classes our model can classify, which classes it confuses with other classes, and if there are any classes that it prefers to predict. There's actually a lot of information in that graph, that can tell us different things about our model and its potential shortcomings.\n",
        "\n",
        "Let's look at a simpler **_Confusion Matrix_** and break down all the information that it displays:\n",
        "\n",
        "<img src=\"./imgs/confusion2.jpg\" height=\"350px\" />\n",
        "\n",
        "The above matrix for a fictitious model shows that we are interested in classifying things into two categories, with labels $0$ and $1$. Our data set contains $7$ instances of the $0$ class (sum of first row), and $10$ instances of class $1$ (sum of second row).\n",
        "\n",
        "Our model correctly predicted $5$ of the $7$ instances of class $0$; while correctly predicting $7$ of the $10$ instances of class $1$. These are called the `True` classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Accuracy\n",
        "\n",
        "We can use these numbers to compute the **_Accuracy_** of our model, or, how often it gets the correct answer:\n",
        "\n",
        "$\\displaystyle Accuracy = \\frac{T_0 + T_1}{Total\\ Instances}$\n",
        "\n",
        "And for our example model:\n",
        "\n",
        "$\\displaystyle Accuracy = \\frac{5 + 7}{5+2+3+7} = \\frac{12}{17} \\approx 0.7059 = 70.6\\%$\n",
        "\n",
        "Besides the **_Accuracy_** of our model, we can also compute **_Precision_** and **_Recall_** characteristics for each of the classes in our model.\n",
        "\n",
        "#### Precision\n",
        "\n",
        "**_Precision_** is the proportion of our predictions that were actually correct for any given class. This number will be low if our model is biased to predict any of the classes.\n",
        "\n",
        "$\\displaystyle Precision_c = \\frac{T_c}{T_c + F_c}$\n",
        "\n",
        "$\\displaystyle Precision_0 = \\frac{5}{5 + 3} = 0.625 = 62.5\\%$ $\\hspace{5em}$\n",
        "$\\displaystyle Precision_1 = \\frac{7}{7 + 2} \\approx 0.7778 = 77.78\\%$\n",
        "\n",
        "#### Recall \n",
        "\n",
        "**_Recall_** is the proportion of the true data labels that were predicted correctly for each class. This number will be low if our model can't really recognize one of the classes.\n",
        "\n",
        "$\\displaystyle Recall_c = \\frac{T_c}{Total_c}$\n",
        "\n",
        "$\\displaystyle Recall_0 = \\frac{5}{5 + 2} \\approx 0.7143 = 71.43\\%$ $\\hspace{5em}$\n",
        "$\\displaystyle Recall_1 = \\frac{7}{7 + 3} = 0.7 = 70\\%$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Many Classes\n",
        "\n",
        "The exact same calculations can be done on any confusion matrix, independent of the number of classes in our model.\n",
        "\n",
        "Consider the following matrix for a classification model with $3$ classes:\n",
        "\n",
        "<img src=\"./imgs/confusion3.jpg\" height=\"350px\" />\n",
        "\n",
        "#### Accuracy\n",
        "\n",
        "This is still $\\frac{correct}{total}$. So, for our model above:\n",
        "\n",
        "$\\displaystyle Accuracy = \\frac{5 + 7 + 6}{5+2+1+2+7+2+3+0+6} = \\frac{18}{28} \\approx 0.6428 = 64.3\\%$\n",
        "\n",
        "#### Precision\n",
        "\n",
        "The proportion of our predictions that were actually correct for any given class.\n",
        "\n",
        "$\\displaystyle Precision_0 = \\frac{5}{5 + 2 + 3} = 0.5 = 50\\%$ $\\hspace{5em}$\n",
        "$\\displaystyle Precision_1 = \\frac{7}{7 + 2 + 0} \\approx 0.7778 = 77.78\\%$\n",
        "\n",
        "$\\displaystyle Precision_2 = \\frac{6}{6 + 2 + 1} \\approx 0.6667 = 66.67\\%$\n",
        "\n",
        "#### Recall\n",
        "\n",
        "The proportion of the true data labels that were predicted correctly for each class.\n",
        "\n",
        "$\\displaystyle Recall_0 = \\frac{5}{5 + 2 + 1} = 0.625 = 62.5\\%$ $\\hspace{5em}$\n",
        "$\\displaystyle Recall_1 = \\frac{7}{7 + 2 + 2} \\approx 0.6364 = 63.64\\%$\n",
        "\n",
        "$\\displaystyle Recall_2 = \\frac{6}{6 + 0 + 3} \\approx 0.6667 = 66.67\\%$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluations\n",
        "\n",
        "Ideally we would have $100\\%$ for all of these metrics, but that's rarely the case.\n",
        "\n",
        "They become useful when we have to consider tradeoffs between having models that are too sensitive and eager to predict certain classes, which might lead to low **_precision_**, versus models that don't predict certain classes ever, which would lead to low **_recall_**.\n",
        "\n",
        "For example, if we are working on a face recognition system that will be used to unlock people's devices, we can probably live with lower **_recall_**, as long as our **_precision_** is high. This would mean that the system wouldn't always recognize certain faces easily, making it hard for some people to unlock their devices, but, on the other hand, the high **_precision_** would mean that it would be rare for the system to unlock the device for the wrong face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wine Confusion\n",
        "\n",
        "Let's look at the confusion matrices for our wine model.\n",
        "\n",
        "First, the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_labels = [q for q in range(wines_df[\"quality\"].min(), wines_df[\"quality\"].max()+1)]\n",
        "\n",
        "display_confusion_matrix(wines_df[\"quality\"], predicted_train[\"quality\"], display_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_confusion_matrix(wines_test_df[\"quality\"], predicted_test[\"quality\"], display_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating Accuracy, Precision, Recall\n",
        "\n",
        "Let's... use the `Scikit-Learn` metric functions to evaluate our model.\n",
        "\n",
        "We can use `accuracy_score()`, `precision_score()` and `recall_score()` by just passing our $2$ lists of true labels and predicted labels.\n",
        "\n",
        "For calculating multi-class `precision_score()` and `recall_score()` we also have to specify how the class precisions should be combined into a final score, if at all.\n",
        "\n",
        "If we want to see per-class precision and recall, we just have to pass `average=None` as a parameter to `precision_score()` and `recall_score()`.\n",
        "\n",
        "Sometimes we also have to tell the function what to do to avoid a division-by-zero when a row/column has no predictions. We do that by parring `zero_division=0` and telling it to assign a precision/recall score of $0$ whenever a row/column has no predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(wines_df[\"quality\"].values, predicted_train[\"quality\"].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_classification_scores(labels, predictions):\n",
        "  print(\"Accuracy:\", accuracy_score(labels, predictions))\n",
        "  print(\"Precision:\", precision_score(labels, predictions, average=None, zero_division=0).round(3))\n",
        "  print(\"Recall:\", recall_score(labels, predictions, average=None, zero_division=0).round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train\")\n",
        "print_classification_scores(wines_df[\"quality\"].values, predicted_train[\"quality\"].values)\n",
        "\n",
        "print(\"Test\")\n",
        "print_classification_scores(wines_test_df[\"quality\"].values, predicted_test[\"quality\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train\")\n",
        "print(classification_report(wines_df[\"quality\"].values, predicted_train[\"quality\"].values, zero_division=0))\n",
        "\n",
        "print(\"Test\")\n",
        "print(classification_report(wines_test_df[\"quality\"].values, predicted_test[\"quality\"].values, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "We have an *ok* model for predicting wine quality based on a few parameters.\n",
        "\n",
        "The scatter plots and the confusion matrix for the test dataset show that our model doesn't perform very well for the $2$ lowest and $2$ highest quality groups. This can be because of the scarcity of data points for those groups, as it never learns to even predict $3$ or $9$.\n",
        "\n",
        "The confusion matrix also shows that the model has a hard time with group $7$, where it predicts group $6$ about $40\\%$ of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterate\n",
        "\n",
        "Repeat the classification, but using one of the other modeling techniques: `SGDClassifier`, `SVC` or `LogisticRegression`.\n",
        "\n",
        "The data is already scaled and in separate `DataFrame`s. We just have to:\n",
        "- Create a Classifier object\n",
        "- Run `fit()` on the training data\n",
        "- Evaluate using the training and test datasets\n",
        "\n",
        "Does it make sense to pick one over the others ? Is it easier, harder or the same to set up and pick parameters ? Are the results very different ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: re-fit the data using a different modeling technique\n",
        "\n",
        "## 5. Create a new Classifier object\n",
        "model_log = LogisticRegression(max_iter=1000, random_state=1010)\n",
        "model_log.fit(feats, wines_df[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_train = model_log.predict(feats)\n",
        "\n",
        "## 7. Measure error\n",
        "display(regression_error(wines_df[\"quality\"], predicted_train))\n",
        "\n",
        "## 7b. Look at graphs\n",
        "plot_preds(wines_df, predicted_train, feat_list, \"quality\")\n",
        "\n",
        "## 8. Look at Confusion Matrices\n",
        "display_confusion_matrix(wines_df[\"quality\"], predicted_train[\"quality\"], display_labels)\n",
        "\n",
        "## 8b. Print scores\n",
        "print(\"Train\")\n",
        "print(classification_report(wines_df[\"quality\"].values, predicted_train[\"quality\"].values, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Run the model on the training data\n",
        "predicted_test = model_log.predict(features_test)\n",
        "\n",
        "## 7. Measure error\n",
        "display(regression_error(wines_test_df[\"quality\"], predicted_test))\n",
        "\n",
        "## 7b. Look at graphs\n",
        "plot_preds(wines_test_df, predicted_test, feat_list, \"quality\")\n",
        "\n",
        "## 8. Look at Confusion Matrices\n",
        "display_confusion_matrix(wines_test_df[\"quality\"], predicted_test[\"quality\"], display_labels)\n",
        "\n",
        "## 8b. Print scores\n",
        "print(\"Test\")\n",
        "print(classification_report(wines_test_df[\"quality\"].values, predicted_test[\"quality\"].values, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Probabilities\n",
        "\n",
        "# TODO HERE\n",
        "\n",
        "# then amend commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_log.predict_proba(feats[:4].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "# TODO HERE\n",
        "# then amend commit"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
